---
title: 'Python for Data Analysis'
description: 'Get started with Python, pandas, and data analysis workflows'
order: 2
duration: '120 minutes'
prerequisites: ['start-here']
---

# Python for Data Analysis

You're about to learn the same tools used by data scientists at Netflix, Airbnb, and Google. By the end, you'll analyze real data, create visualizations, and actually understand what your code is doing.

<Diagram title="What You'll Build" type="flowchart" caption="A complete analysis pipeline">
{`flowchart LR
    A[Raw CSV] --> B[pandas]
    B --> C[Clean Data]
    C --> D[Analysis]
    D --> E[Visualizations]
    E --> F[Insights]
`}
</Diagram>

## Prerequisites

Before starting, make sure you have:

<Checklist>
  <ChecklistItem>Python installed ([Mac](/start-here/mac-setup) or [Windows](/start-here/windows-setup))</ChecklistItem>
  <ChecklistItem>VS Code with Claude Code extension</ChecklistItem>
  <ChecklistItem>Basic terminal knowledge (cd, ls, mkdir)</ChecklistItem>
</Checklist>

---

## Part 1: Project Setup

Every analysis starts with a clean, organized project. Let's build one.

<Steps>
<Step title="Create Project Folder">

```bash
mkdir sales-analysis
cd sales-analysis
```

</Step>
<Step title="Initialize Git">

```bash
git init
```

Track your work from the start. You'll thank yourself later.

</Step>
<Step title="Create Virtual Environment">

Virtual environments isolate your project's packages. No more "it works on my machine" problems.

<Tabs>
  <TabList>
    <Tab>Mac/Linux</Tab>
    <Tab>Windows</Tab>
  </TabList>
  <TabPanel>
```bash
python3 -m venv venv
source venv/bin/activate
```
  </TabPanel>
  <TabPanel>
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```
  </TabPanel>
</Tabs>

You'll see `(venv)` in your prompt—that means it's active.

</Step>
<Step title="Install the Data Science Stack">

```bash
pip install pandas numpy matplotlib jupyter seaborn
```

| Package | What It Does |
|---------|--------------|
| pandas | Data manipulation (your main tool) |
| numpy | Numerical operations |
| matplotlib | Basic plotting |
| seaborn | Beautiful statistical plots |
| jupyter | Interactive notebooks |

</Step>
<Step title="Create Project Structure">

```bash
mkdir data notebooks scripts outputs
touch README.md CLAUDE.md .gitignore
```

<FileTree>
  <FolderNode name="sales-analysis" defaultOpen>
    <FolderNode name="data">
      <FileNode name="sales.csv" />
    </FolderNode>
    <FolderNode name="notebooks">
      <FileNode name="exploration.ipynb" />
    </FolderNode>
    <FolderNode name="scripts">
      <FileNode name="clean.py" />
      <FileNode name="analyze.py" />
    </FolderNode>
    <FolderNode name="outputs">
      <FileNode name="monthly_revenue.png" />
    </FolderNode>
    <FileNode name="README.md" />
    <FileNode name="CLAUDE.md" highlight />
    <FileNode name=".gitignore" />
  </FolderNode>
</FileTree>

</Step>
<Step title="Set Up .gitignore">

Don't commit things that shouldn't be committed:

```
venv/
__pycache__/
*.pyc
.ipynb_checkpoints/
data/*.csv
!data/sample.csv
outputs/
.env
```

</Step>
</Steps>

<Callout type="tip">
**Ask Claude to generate your CLAUDE.md:**
```
Create a CLAUDE.md for a sales data analysis project using Python and pandas.
Include common commands and example prompts.
```
</Callout>

---

## Part 2: Loading Data

Let's load some data and see what we're working with.

### The Core Workflow

<Diagram title="Data Loading Flow" type="flowchart">
{`flowchart LR
    A[CSV File] --> B[pd.read_csv]
    B --> C[DataFrame]
    C --> D[.head/.info/.describe]
    D --> E[Understand your data]
`}
</Diagram>

### Your First Script

Create `scripts/load_data.py`:

```python
import pandas as pd

# Load the data
df = pd.read_csv("data/sales.csv")

# What do we have?
print(f"Rows: {len(df):,}")
print(f"Columns: {list(df.columns)}")
print()

# First look
print(df.head())
print()

# Data types and missing values
print(df.info())
```

### Run It

```bash
python scripts/load_data.py
```

<Callout type="tip">
**No data yet?** Ask Claude:
```
Generate sample sales data with columns: date, product, category, quantity, price, region.
Save it as a CSV I can use for practice.
```
</Callout>

---

## Part 3: Cleaning Data

Real data is messy. Here's how to fix common problems.

### Common Issues (And How to Fix Them)

| Problem | Solution |
|---------|----------|
| Missing values | `df.dropna()` or `df.fillna(value)` |
| Wrong data types | `pd.to_datetime(df['date'])` |
| Duplicates | `df.drop_duplicates()` |
| Inconsistent text | `df['name'].str.lower().str.strip()` |
| Outliers | Filter or cap values |

### Ask Claude to Clean Your Data

```
I have sales data with these issues:
- Date column is a string like "Jan 15, 2024"
- Some revenue values are missing
- Product names have inconsistent capitalization
- There are duplicate rows

Write a cleaning function that fixes all of these.
```

### Example Cleaning Script

```python
import pandas as pd

def clean_sales_data(df):
    """Clean the raw sales data."""
    # Make a copy (don't modify original)
    df = df.copy()

    # Fix dates
    df['date'] = pd.to_datetime(df['date'])

    # Handle missing revenue (fill with median)
    df['revenue'] = df['revenue'].fillna(df['revenue'].median())

    # Standardize product names
    df['product'] = df['product'].str.lower().str.strip()

    # Remove duplicates
    df = df.drop_duplicates()

    # Remove obvious errors (negative quantities)
    df = df[df['quantity'] > 0]

    return df

# Usage
df = pd.read_csv("data/sales.csv")
df_clean = clean_sales_data(df)
print(f"Rows before: {len(df)}, after: {len(df_clean)}")
```

---

## Part 4: Exploring Data

Before you analyze, you need to understand. Here's the exploration toolkit.

### Quick Summary

```python
# Overview
df.describe()           # Statistics for numeric columns
df.info()               # Data types, missing values
df.shape                # (rows, columns)

# Specific columns
df['category'].value_counts()    # Count by category
df['revenue'].mean()             # Average revenue
df['date'].min(), df['date'].max()  # Date range
```

### Grouping and Aggregation

This is where pandas shines:

```python
# Total revenue by product
df.groupby('product')['revenue'].sum()

# Multiple aggregations
df.groupby('category').agg({
    'revenue': 'sum',
    'quantity': 'mean',
    'order_id': 'count'
})

# Monthly totals
df.groupby(df['date'].dt.month)['revenue'].sum()
```

### Ask Claude for Exploration Help

```
I have sales data with: date, product, category, quantity, price, region.

What are the most important things to explore first?
Write the pandas code for each.
```

---

## Part 5: Visualizations

A good chart is worth a thousand `.head()` calls.

<Diagram title="Choosing the Right Chart" type="flowchart">
{`flowchart TB
    A{What are you showing?}
    A -->|Trend over time| B[Line Chart]
    A -->|Compare categories| C[Bar Chart]
    A -->|Distribution| D[Histogram]
    A -->|Relationship| E[Scatter Plot]
    A -->|Composition| F[Pie Chart]
`}
</Diagram>

### Basic Plots

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Set style
plt.style.use('seaborn-v0_8-whitegrid')

# Line chart (trends)
df.groupby('date')['revenue'].sum().plot(kind='line')
plt.title('Daily Revenue')
plt.savefig('outputs/daily_revenue.png')
plt.show()

# Bar chart (comparisons)
df.groupby('category')['revenue'].sum().plot(kind='bar')
plt.title('Revenue by Category')
plt.savefig('outputs/category_revenue.png')
plt.show()

# Histogram (distribution)
df['revenue'].hist(bins=30)
plt.title('Revenue Distribution')
plt.savefig('outputs/revenue_dist.png')
plt.show()
```

### Ask Claude for Better Charts

```
Create a visualization showing monthly revenue trends.

Make it:
- Easy to read (larger fonts)
- Professional looking (clean style)
- Saved as PNG at 300 DPI
- Include a trend line
```

---

## Part 6: Putting It Together

Let's build a complete analysis workflow.

### The Full Script

```python
"""
Sales Analysis Pipeline
Run with: python scripts/analyze.py
"""
import pandas as pd
import matplotlib.pyplot as plt

# 1. Load
print("Loading data...")
df = pd.read_csv("data/sales.csv")

# 2. Clean
print("Cleaning data...")
df['date'] = pd.to_datetime(df['date'])
df = df.dropna()
df = df[df['quantity'] > 0]

# 3. Analyze
print("Analyzing...")
monthly = df.groupby(df['date'].dt.to_period('M')).agg({
    'revenue': 'sum',
    'quantity': 'sum',
    'order_id': 'count'
}).rename(columns={'order_id': 'num_orders'})

# 4. Visualize
print("Creating charts...")
fig, ax = plt.subplots(figsize=(10, 6))
monthly['revenue'].plot(ax=ax)
ax.set_title('Monthly Revenue', fontsize=14)
ax.set_xlabel('Month')
ax.set_ylabel('Revenue ($)')
plt.tight_layout()
plt.savefig('outputs/monthly_revenue.png', dpi=300)

# 5. Report
print("\n=== KEY FINDINGS ===")
print(f"Total Revenue: ${monthly['revenue'].sum():,.2f}")
print(f"Best Month: {monthly['revenue'].idxmax()}")
print(f"Average Monthly Revenue: ${monthly['revenue'].mean():,.2f}")
print(f"\nChart saved to outputs/monthly_revenue.png")
```

### Commit Your Work

```bash
git add scripts/analyze.py outputs/
git commit -m "feat: add complete sales analysis pipeline"
```

---

## Part 7: Jupyter Notebooks

For interactive exploration, Jupyter notebooks are your friend.

### Start Jupyter

```bash
jupyter notebook
```

This opens a browser. Create a new Python 3 notebook.

### Notebook Best Practices

| Do | Don't |
|----|-------|
| Use markdown headers | Run cells out of order |
| Keep cells small and focused | Put all code in one cell |
| Restart and run all before sharing | Leave broken cells |
| Include explanatory text | Assume code is self-explanatory |

<Callout type="tip">
**Ask Claude to structure your notebook:**
```
Create a Jupyter notebook structure for analyzing customer purchase patterns.
Include markdown sections and placeholder code cells.
```
</Callout>

---

## Common Patterns

Keep these in your back pocket:

### Filter Rows

```python
# Single condition
high_revenue = df[df['revenue'] > 1000]

# Multiple conditions
q4_big_sales = df[(df['date'].dt.quarter == 4) & (df['revenue'] > 500)]
```

### Create New Columns

```python
# From calculation
df['profit'] = df['revenue'] - df['cost']
df['profit_margin'] = df['profit'] / df['revenue']

# From date
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.day_name()

# From categories
df['size'] = pd.cut(df['revenue'], bins=[0, 100, 500, float('inf')],
                    labels=['small', 'medium', 'large'])
```

### Join DataFrames

```python
# Merge on common column
df_full = pd.merge(orders, customers, on='customer_id')

# Concatenate vertically
all_months = pd.concat([jan_df, feb_df, mar_df])
```

---

## Troubleshooting

<Tabs>
  <TabList>
    <Tab>Import Errors</Tab>
    <Tab>Data Type Issues</Tab>
    <Tab>Memory Problems</Tab>
  </TabList>
  <TabPanel>
**"ModuleNotFoundError: No module named 'pandas'"**

Your virtual environment isn't active or packages aren't installed.

```bash
# Check if venv is active (should see (venv) in prompt)
which python  # Should point to venv/bin/python

# If not, activate it
source venv/bin/activate  # Mac/Linux
.\venv\Scripts\Activate.ps1  # Windows

# Reinstall
pip install pandas numpy matplotlib
```
  </TabPanel>
  <TabPanel>
**Date/number parsing issues**

```python
# Force date parsing
df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')

# Handle mixed formats
df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)

# Convert to numeric (coerce errors to NaN)
df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce')
```
  </TabPanel>
  <TabPanel>
**Working with large files**

```python
# Read in chunks
chunks = pd.read_csv('huge_file.csv', chunksize=100000)
for chunk in chunks:
    process(chunk)

# Read only needed columns
df = pd.read_csv('data.csv', usecols=['date', 'revenue', 'product'])

# Use efficient data types
df['category'] = df['category'].astype('category')
```
  </TabPanel>
</Tabs>

---

## Next Steps

You've got the fundamentals. Here's where to go deeper:

| Want to... | Learn |
|------------|-------|
| Build dashboards | Streamlit, Dash |
| More statistics | scipy, statsmodels |
| Machine learning | scikit-learn |
| Bigger data | Dask, PySpark |
| Automate reports | See [Automation Track](/automation) |

---

## Resources

- [pandas documentation](https://pandas.pydata.org/docs/) — Official reference
- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) — Free online book
- [Kaggle](https://www.kaggle.com/) — Practice datasets and competitions
- [Real Python](https://realpython.com/) — Quality tutorials

---

<Callout type="success">
**You're ready!** Grab a dataset, open VS Code, and start exploring. When you get stuck, Claude is there to help.
</Callout>
